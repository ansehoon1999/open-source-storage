{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbRxrEVKlFvL"
   },
   "source": [
    "#one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gDjhpIoBlFcG"
   },
   "outputs": [],
   "source": [
    "## no library\n",
    "def one_hot(word_list):\n",
    "  #(1) 단어의 중복을 제거해﻿줍니다.\n",
    "  word_list = list(set(word_list))\n",
    "  #(2) 단어의 수만큼 배열을 만들고, 0으로 채워﻿줍니다.\n",
    "  encoding_matrix = [[0 for col in range(len(word_list))] for row in range(len(word_list))]\n",
    "  #(3) 해당 단어의 인덱스를 찾고, 그 부분을 1로 만들어﻿줍니다.\n",
    "  for index, word in enumerate(word_list):\n",
    "    encoding_matrix[index][index] = 1\n",
    "  return encoding_matrix\n",
    "\n",
    "labels = ['cat','dog','rabbit','turtle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsnGG9lflFZp",
    "outputId": "a61a7e40-f5e2-427e-a123-534fac562063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat  dog  rabbit  turtle\n",
      "0    1    0       0       0\n",
      "1    0    1       0       0\n",
      "2    0    0       1       0\n",
      "3    0    0       0       1\n"
     ]
    }
   ],
   "source": [
    "## using pandas\n",
    "import pandas as pd\n",
    "\n",
    "label_dict = {'label':['cat','dog','rabbit','turtle']}\n",
    "#df = pd.DataFrame(label_dict)\n",
    "one_hot_encoding = pd.get_dummies(label_dict['label'])\n",
    "print(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCUKKwYUlFXX",
    "outputId": "ac49d0b9-e792-4719-8777-e758b33df6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "## using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "label_dict = {'label':['cat','dog','rabbit','turtle']}\n",
    "df = pd.DataFrame(label_dict)\n",
    "one_hot = OneHotEncoder()\n",
    "one_hot_encoding = one_hot.fit_transform(df)\n",
    "print(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBERZMTUlSij"
   },
   "source": [
    "#BoW(Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JJoaRawlFUw",
    "outputId": "7377537c-f84e-47a0-b70e-13c2ef13c2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list :  ['and', 'very', 'Suzy', 'too', 'is', 'YoonA', 'woman', 'pretty'] , embedding :  [1, 3, 1, 1, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "## no library\n",
    "def bow(sentence):\n",
    "  #(1) 입력받은 문장을 단어 단위로 쪼갠 뒤, 중복을 제거해﻿줍니다.\n",
    "  word_list = sentence.split(' ')\n",
    "  word_list = list(set(word_list))\n",
    "  #(2) 단어의 수만큼 배열을 만들고, 0으로 채워﻿줍니다.\n",
    "  embedding_matrix = [0 for element in range(len(word_list))]\n",
    "  #(3) 각 인덱스의 단어가 몇 번 나오는지 count한뒤, 갱신해﻿줍니다.\n",
    "  for index, word in enumerate(word_list):\n",
    "    embedding_matrix[index] = sentence.count(word)\n",
    "  return word_list, embedding_matrix\n",
    "\n",
    "sentence = \"Suzy is very very pretty woman and YoonA is very pretty woman too\"\n",
    "word_list, bow_embedding = bow(sentence)\n",
    "print(\"word_list : \",word_list,\", embedding : \",bow_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZ1hiOEYlFSe",
    "outputId": "4486076c-2e4a-40b7-e17d-97f33b1ea8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list :  ['and', 'is', 'pretty', 'suzy', 'too', 'very', 'woman', 'yoona'] , embedding :  [[1 2 2 1 1 3 2 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## using sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = [\"Suzy is very very pretty woman and YoonA is very pretty woman too\"]\n",
    "vectorizer = CountVectorizer(min_df = 1, ngram_range = (1,1))\n",
    "embedding = vectorizer.fit_transform(sentence)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(\"word_list : \",vocab,\", embedding : \",embedding.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E39y-RTglbWp"
   },
   "source": [
    "#cBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Vy95tnerlk75"
   },
   "outputs": [],
   "source": [
    "# convert context to index vector\n",
    "def make_context_vector(context, word_to_ix):\n",
    "  idxs = [word_to_ix[w] for w in context]\n",
    "  return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# make dataset function\n",
    "def make_data(sentence):\n",
    "  data = []\n",
    "  for i in range(2, len(example_sentence) - 2):\n",
    "    context = [example_sentence[i - 2], example_sentence[i - 1], example_sentence[i + 1], example_sentence[i + 2]]\n",
    "    target = example_sentence[i]\n",
    "    data.append((context, target))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-QEe4wORlrY-"
   },
   "outputs": [],
   "source": [
    "#(4) CBOW 모델을 정의해 줍니다.\n",
    "class CBOW(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(CBOW, self).__init__()\n",
    "\n",
    "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    self.layer1 = nn.Linear(embedding_dim, 64)\n",
    "    self.activation1 = nn.ReLU()\n",
    "\n",
    "    self.layer2 = nn.Linear(64, vocab_size)\n",
    "    self.activation2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embeded_vector = sum(self.embeddings(inputs)).view(1,-1)\n",
    "    output = self.activation1(self.layer1(embeded_vector))\n",
    "    output = self.activation2(self.layer2(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gq9Hy2I9lFQH",
    "outputId": "6fd4d6e2-9325-4252-8383-dbb233c74b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'case', 'of', 'CBOW,', 'one', 'word', 'is', 'eliminated,', 'and', 'the', 'word', 'is', 'predicted', 'from', 'surrounding', 'words.', 'Therefore,', 'it', 'takes', 'multiple', 'input', 'vectors', 'as', 'inputs', 'to', 'the', 'model', 'and', 'creates', 'one', 'output', 'vector.', 'In', 'contrast,', 'Skip-Gram', 'learns', 'by', 'removing', 'all', 'words', 'except', 'one', 'word', 'and', 'predicting', 'the', 'surrounding', 'words', 'in', 'the', 'context', 'through', 'one', 'word.', 'So,', 'it', 'takes', 'a', 'vector', 'as', 'input', 'and', 'produces', 'multiple', 'output', 'vectors.', 'CBOW', 'and', 'Skip-Gram', 'are', 'different.']\n",
      "epoch =  0 , loss =  tensor(296.6661, grad_fn=<AddBackward0>)\n",
      "epoch =  1 , loss =  tensor(287.5806, grad_fn=<AddBackward0>)\n",
      "epoch =  2 , loss =  tensor(278.9847, grad_fn=<AddBackward0>)\n",
      "epoch =  3 , loss =  tensor(270.7304, grad_fn=<AddBackward0>)\n",
      "epoch =  4 , loss =  tensor(262.7270, grad_fn=<AddBackward0>)\n",
      "epoch =  5 , loss =  tensor(254.9600, grad_fn=<AddBackward0>)\n",
      "epoch =  6 , loss =  tensor(247.3369, grad_fn=<AddBackward0>)\n",
      "epoch =  7 , loss =  tensor(239.7716, grad_fn=<AddBackward0>)\n",
      "epoch =  8 , loss =  tensor(232.2176, grad_fn=<AddBackward0>)\n",
      "epoch =  9 , loss =  tensor(224.5888, grad_fn=<AddBackward0>)\n",
      "epoch =  10 , loss =  tensor(216.9380, grad_fn=<AddBackward0>)\n",
      "epoch =  11 , loss =  tensor(209.3789, grad_fn=<AddBackward0>)\n",
      "epoch =  12 , loss =  tensor(201.9030, grad_fn=<AddBackward0>)\n",
      "epoch =  13 , loss =  tensor(194.5625, grad_fn=<AddBackward0>)\n",
      "epoch =  14 , loss =  tensor(187.3731, grad_fn=<AddBackward0>)\n",
      "epoch =  15 , loss =  tensor(180.3170, grad_fn=<AddBackward0>)\n",
      "epoch =  16 , loss =  tensor(173.4122, grad_fn=<AddBackward0>)\n",
      "epoch =  17 , loss =  tensor(166.5878, grad_fn=<AddBackward0>)\n",
      "epoch =  18 , loss =  tensor(159.9040, grad_fn=<AddBackward0>)\n",
      "epoch =  19 , loss =  tensor(153.3012, grad_fn=<AddBackward0>)\n",
      "epoch =  20 , loss =  tensor(146.8092, grad_fn=<AddBackward0>)\n",
      "epoch =  21 , loss =  tensor(140.3562, grad_fn=<AddBackward0>)\n",
      "epoch =  22 , loss =  tensor(134.0317, grad_fn=<AddBackward0>)\n",
      "epoch =  23 , loss =  tensor(127.8162, grad_fn=<AddBackward0>)\n",
      "epoch =  24 , loss =  tensor(121.7462, grad_fn=<AddBackward0>)\n",
      "epoch =  25 , loss =  tensor(115.8278, grad_fn=<AddBackward0>)\n",
      "epoch =  26 , loss =  tensor(110.0682, grad_fn=<AddBackward0>)\n",
      "epoch =  27 , loss =  tensor(104.4898, grad_fn=<AddBackward0>)\n",
      "epoch =  28 , loss =  tensor(99.0597, grad_fn=<AddBackward0>)\n",
      "epoch =  29 , loss =  tensor(93.8049, grad_fn=<AddBackward0>)\n",
      "epoch =  30 , loss =  tensor(88.7401, grad_fn=<AddBackward0>)\n",
      "epoch =  31 , loss =  tensor(83.8660, grad_fn=<AddBackward0>)\n",
      "epoch =  32 , loss =  tensor(79.2026, grad_fn=<AddBackward0>)\n",
      "epoch =  33 , loss =  tensor(74.7238, grad_fn=<AddBackward0>)\n",
      "epoch =  34 , loss =  tensor(70.4573, grad_fn=<AddBackward0>)\n",
      "epoch =  35 , loss =  tensor(66.4068, grad_fn=<AddBackward0>)\n",
      "epoch =  36 , loss =  tensor(62.5747, grad_fn=<AddBackward0>)\n",
      "epoch =  37 , loss =  tensor(58.9409, grad_fn=<AddBackward0>)\n",
      "epoch =  38 , loss =  tensor(55.5228, grad_fn=<AddBackward0>)\n",
      "epoch =  39 , loss =  tensor(52.3115, grad_fn=<AddBackward0>)\n",
      "epoch =  40 , loss =  tensor(49.2728, grad_fn=<AddBackward0>)\n",
      "epoch =  41 , loss =  tensor(46.4393, grad_fn=<AddBackward0>)\n",
      "epoch =  42 , loss =  tensor(43.7650, grad_fn=<AddBackward0>)\n",
      "epoch =  43 , loss =  tensor(41.2839, grad_fn=<AddBackward0>)\n",
      "epoch =  44 , loss =  tensor(38.9641, grad_fn=<AddBackward0>)\n",
      "epoch =  45 , loss =  tensor(36.7990, grad_fn=<AddBackward0>)\n",
      "epoch =  46 , loss =  tensor(34.7949, grad_fn=<AddBackward0>)\n",
      "epoch =  47 , loss =  tensor(32.9302, grad_fn=<AddBackward0>)\n",
      "epoch =  48 , loss =  tensor(31.1998, grad_fn=<AddBackward0>)\n",
      "epoch =  49 , loss =  tensor(29.5916, grad_fn=<AddBackward0>)\n",
      "epoch =  50 , loss =  tensor(28.1010, grad_fn=<AddBackward0>)\n",
      "epoch =  51 , loss =  tensor(26.7149, grad_fn=<AddBackward0>)\n",
      "epoch =  52 , loss =  tensor(25.4322, grad_fn=<AddBackward0>)\n",
      "epoch =  53 , loss =  tensor(24.2354, grad_fn=<AddBackward0>)\n",
      "epoch =  54 , loss =  tensor(23.1299, grad_fn=<AddBackward0>)\n",
      "epoch =  55 , loss =  tensor(22.0941, grad_fn=<AddBackward0>)\n",
      "epoch =  56 , loss =  tensor(21.1319, grad_fn=<AddBackward0>)\n",
      "epoch =  57 , loss =  tensor(20.2344, grad_fn=<AddBackward0>)\n",
      "epoch =  58 , loss =  tensor(19.3943, grad_fn=<AddBackward0>)\n",
      "epoch =  59 , loss =  tensor(18.6117, grad_fn=<AddBackward0>)\n",
      "epoch =  60 , loss =  tensor(17.8754, grad_fn=<AddBackward0>)\n",
      "epoch =  61 , loss =  tensor(17.1894, grad_fn=<AddBackward0>)\n",
      "epoch =  62 , loss =  tensor(16.5454, grad_fn=<AddBackward0>)\n",
      "epoch =  63 , loss =  tensor(15.9400, grad_fn=<AddBackward0>)\n",
      "epoch =  64 , loss =  tensor(15.3727, grad_fn=<AddBackward0>)\n",
      "epoch =  65 , loss =  tensor(14.8363, grad_fn=<AddBackward0>)\n",
      "epoch =  66 , loss =  tensor(14.3322, grad_fn=<AddBackward0>)\n",
      "epoch =  67 , loss =  tensor(13.8561, grad_fn=<AddBackward0>)\n",
      "epoch =  68 , loss =  tensor(13.4071, grad_fn=<AddBackward0>)\n",
      "epoch =  69 , loss =  tensor(12.9822, grad_fn=<AddBackward0>)\n",
      "epoch =  70 , loss =  tensor(12.5797, grad_fn=<AddBackward0>)\n",
      "epoch =  71 , loss =  tensor(12.1991, grad_fn=<AddBackward0>)\n",
      "epoch =  72 , loss =  tensor(11.8361, grad_fn=<AddBackward0>)\n",
      "epoch =  73 , loss =  tensor(11.4924, grad_fn=<AddBackward0>)\n",
      "epoch =  74 , loss =  tensor(11.1651, grad_fn=<AddBackward0>)\n",
      "epoch =  75 , loss =  tensor(10.8537, grad_fn=<AddBackward0>)\n",
      "epoch =  76 , loss =  tensor(10.5576, grad_fn=<AddBackward0>)\n",
      "epoch =  77 , loss =  tensor(10.2747, grad_fn=<AddBackward0>)\n",
      "epoch =  78 , loss =  tensor(10.0051, grad_fn=<AddBackward0>)\n",
      "epoch =  79 , loss =  tensor(9.7476, grad_fn=<AddBackward0>)\n",
      "epoch =  80 , loss =  tensor(9.5010, grad_fn=<AddBackward0>)\n",
      "epoch =  81 , loss =  tensor(9.2650, grad_fn=<AddBackward0>)\n",
      "epoch =  82 , loss =  tensor(9.0394, grad_fn=<AddBackward0>)\n",
      "epoch =  83 , loss =  tensor(8.8231, grad_fn=<AddBackward0>)\n",
      "epoch =  84 , loss =  tensor(8.6157, grad_fn=<AddBackward0>)\n",
      "epoch =  85 , loss =  tensor(8.4169, grad_fn=<AddBackward0>)\n",
      "epoch =  86 , loss =  tensor(8.2258, grad_fn=<AddBackward0>)\n",
      "epoch =  87 , loss =  tensor(8.0422, grad_fn=<AddBackward0>)\n",
      "epoch =  88 , loss =  tensor(7.8661, grad_fn=<AddBackward0>)\n",
      "epoch =  89 , loss =  tensor(7.6962, grad_fn=<AddBackward0>)\n",
      "epoch =  90 , loss =  tensor(7.5334, grad_fn=<AddBackward0>)\n",
      "epoch =  91 , loss =  tensor(7.3758, grad_fn=<AddBackward0>)\n",
      "epoch =  92 , loss =  tensor(7.2242, grad_fn=<AddBackward0>)\n",
      "epoch =  93 , loss =  tensor(7.0784, grad_fn=<AddBackward0>)\n",
      "epoch =  94 , loss =  tensor(6.9376, grad_fn=<AddBackward0>)\n",
      "epoch =  95 , loss =  tensor(6.8018, grad_fn=<AddBackward0>)\n",
      "epoch =  96 , loss =  tensor(6.6704, grad_fn=<AddBackward0>)\n",
      "epoch =  97 , loss =  tensor(6.5437, grad_fn=<AddBackward0>)\n",
      "epoch =  98 , loss =  tensor(6.4217, grad_fn=<AddBackward0>)\n",
      "epoch =  99 , loss =  tensor(6.3031, grad_fn=<AddBackward0>)\n",
      "Prediction :  Skip-Gram\n"
     ]
    }
   ],
   "source": [
    "## using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "with open('../[01]data_set/data_set.txt', 'r') as file: \n",
    "    example_sentence = file.readline()\n",
    "\n",
    "example_sentence = example_sentence.split()\n",
    "print(example_sentence)\n",
    "\n",
    "\n",
    "#(1) 입력받은 문장을 단어로 쪼개고, 중복을 제거해줍니다.\n",
    "vocab = set(example_sentence)\n",
    "vocab_size = len(example_sentence)\n",
    "\n",
    "#(2) 단어 : 인덱스, 인덱스 : 단어를 가지는 딕셔너리를 선언해 줍니다.\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for index, word in enumerate(vocab)}\n",
    "\n",
    "#(3) 학습을 위한 데이터를 생성해 줍니다.\n",
    "data = make_data(example_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#﻿(5) 모델을 선언해주고, loss function, optimizer등을 선언해줍니다.\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "#﻿(6) 학습을 진행합니다.\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_index)  \n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_index[target]]))\n",
    "    print('epoch = ',epoch, ', loss = ',total_loss)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#﻿(7) test하고 싶은 문장을 뽑고, test를 진행합니다.\n",
    "test_data = ['CBOW','and','are','different.']\n",
    "test_vector = make_context_vector(test_data, word_to_index)\n",
    "result = model(test_vector)\n",
    "print('Prediction : ', index_to_word[torch.argmax(result[0]).item()])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
