{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.11.2+cu102)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torchvision) (1.19.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: torch==1.10.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torchvision) (1.10.1+cu102)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch==1.10.1->torchvision) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.10.1+cu102  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211219e0b3a4460a828b35086a4af925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75720e548a66465baf026140af24e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e16467ab68485690c11dfbaa848fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e1fe8abb5643ca8e0d08b8393faaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/MNIST\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.139114\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.327335\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.487634\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.096294\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.271720\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.315437\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.444940\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.229398\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.063530\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.425345\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.1392, \tTest Accuracy: 95.78 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.130689\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.238156\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.282381\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.267445\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.132703\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.212889\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.223224\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.216119\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.102152\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.092976\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1033, \tTest Accuracy: 96.88 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.082568\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.166533\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.391452\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.088492\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.079766\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.362378\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.156898\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.202865\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.132699\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.174987\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0934, \tTest Accuracy: 97.01 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.423958\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.782697\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.135523\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.206448\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.337845\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.078179\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.039909\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.109417\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.108537\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.096634\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0891, \tTest Accuracy: 97.26 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.122919\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.267197\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.130671\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.227460\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.356677\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.585642\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.271243\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.569545\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.131935\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.589212\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0872, \tTest Accuracy: 97.23 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.236432\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.157131\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.055736\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.109698\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.113529\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.245963\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.062553\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.129390\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.217385\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.127205\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0708, \tTest Accuracy: 97.67 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.168869\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.018268\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.205438\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.296998\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.093989\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.244159\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.026213\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.017695\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.245915\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.065910\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0750, \tTest Accuracy: 97.87 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.037891\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.173444\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.657488\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.110221\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.114579\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.214854\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.176479\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.266097\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.191569\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.106526\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0675, \tTest Accuracy: 97.78 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.263594\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.096864\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.156184\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.158747\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.189435\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.032973\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.150111\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.088237\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.017809\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.104216\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0681, \tTest Accuracy: 98.03 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.023133\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.120121\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.104642\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.025456\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.077835\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.143424\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.254119\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.178311\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.055804\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.062006\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0628, \tTest Accuracy: 98.14 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
