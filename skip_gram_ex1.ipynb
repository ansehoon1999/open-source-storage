{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbRxrEVKlFvL"
   },
   "source": [
    "#one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gDjhpIoBlFcG"
   },
   "outputs": [],
   "source": [
    "## no library\n",
    "def one_hot(word_list):\n",
    "  #(1) 단어의 중복을 제거해﻿줍니다.\n",
    "  word_list = list(set(word_list))\n",
    "  #(2) 단어의 수만큼 배열을 만들고, 0으로 채워﻿줍니다.\n",
    "  encoding_matrix = [[0 for col in range(len(word_list))] for row in range(len(word_list))]\n",
    "  #(3) 해당 단어의 인덱스를 찾고, 그 부분을 1로 만들어﻿줍니다.\n",
    "  for index, word in enumerate(word_list):\n",
    "    encoding_matrix[index][index] = 1\n",
    "  return encoding_matrix\n",
    "\n",
    "labels = ['cat','dog','rabbit','turtle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsnGG9lflFZp",
    "outputId": "a61a7e40-f5e2-427e-a123-534fac562063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat  dog  rabbit  turtle\n",
      "0    1    0       0       0\n",
      "1    0    1       0       0\n",
      "2    0    0       1       0\n",
      "3    0    0       0       1\n"
     ]
    }
   ],
   "source": [
    "## using pandas\n",
    "import pandas as pd\n",
    "\n",
    "label_dict = {'label':['cat','dog','rabbit','turtle']}\n",
    "#df = pd.DataFrame(label_dict)\n",
    "one_hot_encoding = pd.get_dummies(label_dict['label'])\n",
    "print(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCUKKwYUlFXX",
    "outputId": "ac49d0b9-e792-4719-8777-e758b33df6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "## using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "label_dict = {'label':['cat','dog','rabbit','turtle']}\n",
    "df = pd.DataFrame(label_dict)\n",
    "one_hot = OneHotEncoder()\n",
    "one_hot_encoding = one_hot.fit_transform(df)\n",
    "print(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgz2ImETl7qA"
   },
   "source": [
    "#skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xla98TCJl-6e"
   },
   "outputs": [],
   "source": [
    "# convert context to index vector\n",
    "def make_context_vector(context, word_to_ix):\n",
    "  idxs = word_to_ix[context]\n",
    "  return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# make dataset function\n",
    "def make_data(sentence):\n",
    "  data = []\n",
    "  for i in range(2, len(example_sentence) - 2):\n",
    "    context = example_sentence[i]\n",
    "    target = [example_sentence[i - 2], example_sentence[i - 1], example_sentence[i + 1], example_sentence[i + 2]]\n",
    "    data.append((context, target))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ofYLNOoAl-wm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#(4) Skip-Gram 모델을 정의해 줍니다.\n",
    "class SKIP_GRAM(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "    super(SKIP_GRAM, self).__init__()\n",
    "    self.context_size = context_size\n",
    "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    self.layer1 = nn.Linear(embedding_dim, 64)\n",
    "    self.activation1 = nn.ReLU()\n",
    "\n",
    "    self.layer2 = nn.Linear(64, vocab_size * context_size)\n",
    "    self.activation2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embeded_vector = self.embeddings(inputs)\n",
    "    output = self.activation1(self.layer1(embeded_vector))\n",
    "    output = self.activation2(self.layer2(output))\n",
    "    return output.view(self.context_size,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UjG5aPJlFNw",
    "outputId": "f67a9fbf-142c-4c0f-e66c-03d4f056c83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'case', 'of', 'CBOW,', 'one', 'word', 'is', 'eliminated,', 'and', 'the', 'word', 'is', 'predicted', 'from', 'surrounding', 'words.', 'Therefore,', 'it', 'takes', 'multiple', 'input', 'vectors', 'as', 'inputs', 'to', 'the', 'model', 'and', 'creates', 'one', 'output', 'vector.', 'In', 'contrast,', 'Skip-Gram', 'learns', 'by', 'removing', 'all', 'words', 'except', 'one', 'word', 'and', 'predicting', 'the', 'surrounding', 'words', 'in', 'the', 'context', 'through', 'one', 'word.', 'So,', 'it', 'takes', 'a', 'vector', 'as', 'input', 'and', 'produces', 'multiple', 'output', 'vectors.', 'CBOW', 'and', 'Skip-Gram', 'are', 'different.']\n",
      "epoch =  0 , loss =  tensor(386.0436, grad_fn=<AddBackward0>)\n",
      "epoch =  1 , loss =  tensor(385.4234, grad_fn=<AddBackward0>)\n",
      "epoch =  2 , loss =  tensor(384.8090, grad_fn=<AddBackward0>)\n",
      "epoch =  3 , loss =  tensor(384.2037, grad_fn=<AddBackward0>)\n",
      "epoch =  4 , loss =  tensor(383.6024, grad_fn=<AddBackward0>)\n",
      "epoch =  5 , loss =  tensor(383.0038, grad_fn=<AddBackward0>)\n",
      "epoch =  6 , loss =  tensor(382.4089, grad_fn=<AddBackward0>)\n",
      "epoch =  7 , loss =  tensor(381.8167, grad_fn=<AddBackward0>)\n",
      "epoch =  8 , loss =  tensor(381.2266, grad_fn=<AddBackward0>)\n",
      "epoch =  9 , loss =  tensor(380.6421, grad_fn=<AddBackward0>)\n",
      "epoch =  10 , loss =  tensor(380.0645, grad_fn=<AddBackward0>)\n",
      "epoch =  11 , loss =  tensor(379.4901, grad_fn=<AddBackward0>)\n",
      "epoch =  12 , loss =  tensor(378.9193, grad_fn=<AddBackward0>)\n",
      "epoch =  13 , loss =  tensor(378.3522, grad_fn=<AddBackward0>)\n",
      "epoch =  14 , loss =  tensor(377.7879, grad_fn=<AddBackward0>)\n",
      "epoch =  15 , loss =  tensor(377.2252, grad_fn=<AddBackward0>)\n",
      "epoch =  16 , loss =  tensor(376.6658, grad_fn=<AddBackward0>)\n",
      "epoch =  17 , loss =  tensor(376.1114, grad_fn=<AddBackward0>)\n",
      "epoch =  18 , loss =  tensor(375.5574, grad_fn=<AddBackward0>)\n",
      "epoch =  19 , loss =  tensor(375.0041, grad_fn=<AddBackward0>)\n",
      "epoch =  20 , loss =  tensor(374.4493, grad_fn=<AddBackward0>)\n",
      "epoch =  21 , loss =  tensor(373.8923, grad_fn=<AddBackward0>)\n",
      "epoch =  22 , loss =  tensor(373.3350, grad_fn=<AddBackward0>)\n",
      "epoch =  23 , loss =  tensor(372.7768, grad_fn=<AddBackward0>)\n",
      "epoch =  24 , loss =  tensor(372.2176, grad_fn=<AddBackward0>)\n",
      "epoch =  25 , loss =  tensor(371.6568, grad_fn=<AddBackward0>)\n",
      "epoch =  26 , loss =  tensor(371.0941, grad_fn=<AddBackward0>)\n",
      "epoch =  27 , loss =  tensor(370.5305, grad_fn=<AddBackward0>)\n",
      "epoch =  28 , loss =  tensor(369.9635, grad_fn=<AddBackward0>)\n",
      "epoch =  29 , loss =  tensor(369.3936, grad_fn=<AddBackward0>)\n",
      "epoch =  30 , loss =  tensor(368.8224, grad_fn=<AddBackward0>)\n",
      "epoch =  31 , loss =  tensor(368.2501, grad_fn=<AddBackward0>)\n",
      "epoch =  32 , loss =  tensor(367.6771, grad_fn=<AddBackward0>)\n",
      "epoch =  33 , loss =  tensor(367.1005, grad_fn=<AddBackward0>)\n",
      "epoch =  34 , loss =  tensor(366.5214, grad_fn=<AddBackward0>)\n",
      "epoch =  35 , loss =  tensor(365.9395, grad_fn=<AddBackward0>)\n",
      "epoch =  36 , loss =  tensor(365.3547, grad_fn=<AddBackward0>)\n",
      "epoch =  37 , loss =  tensor(364.7671, grad_fn=<AddBackward0>)\n",
      "epoch =  38 , loss =  tensor(364.1767, grad_fn=<AddBackward0>)\n",
      "epoch =  39 , loss =  tensor(363.5864, grad_fn=<AddBackward0>)\n",
      "epoch =  40 , loss =  tensor(362.9931, grad_fn=<AddBackward0>)\n",
      "epoch =  41 , loss =  tensor(362.3974, grad_fn=<AddBackward0>)\n",
      "epoch =  42 , loss =  tensor(361.7978, grad_fn=<AddBackward0>)\n",
      "epoch =  43 , loss =  tensor(361.1966, grad_fn=<AddBackward0>)\n",
      "epoch =  44 , loss =  tensor(360.5910, grad_fn=<AddBackward0>)\n",
      "epoch =  45 , loss =  tensor(359.9829, grad_fn=<AddBackward0>)\n",
      "epoch =  46 , loss =  tensor(359.3693, grad_fn=<AddBackward0>)\n",
      "epoch =  47 , loss =  tensor(358.7512, grad_fn=<AddBackward0>)\n",
      "epoch =  48 , loss =  tensor(358.1293, grad_fn=<AddBackward0>)\n",
      "epoch =  49 , loss =  tensor(357.5015, grad_fn=<AddBackward0>)\n",
      "epoch =  50 , loss =  tensor(356.8690, grad_fn=<AddBackward0>)\n",
      "epoch =  51 , loss =  tensor(356.2346, grad_fn=<AddBackward0>)\n",
      "epoch =  52 , loss =  tensor(355.5920, grad_fn=<AddBackward0>)\n",
      "epoch =  53 , loss =  tensor(354.9441, grad_fn=<AddBackward0>)\n",
      "epoch =  54 , loss =  tensor(354.2919, grad_fn=<AddBackward0>)\n",
      "epoch =  55 , loss =  tensor(353.6329, grad_fn=<AddBackward0>)\n",
      "epoch =  56 , loss =  tensor(352.9694, grad_fn=<AddBackward0>)\n",
      "epoch =  57 , loss =  tensor(352.2987, grad_fn=<AddBackward0>)\n",
      "epoch =  58 , loss =  tensor(351.6243, grad_fn=<AddBackward0>)\n",
      "epoch =  59 , loss =  tensor(350.9436, grad_fn=<AddBackward0>)\n",
      "epoch =  60 , loss =  tensor(350.2567, grad_fn=<AddBackward0>)\n",
      "epoch =  61 , loss =  tensor(349.5645, grad_fn=<AddBackward0>)\n",
      "epoch =  62 , loss =  tensor(348.8646, grad_fn=<AddBackward0>)\n",
      "epoch =  63 , loss =  tensor(348.1592, grad_fn=<AddBackward0>)\n",
      "epoch =  64 , loss =  tensor(347.4467, grad_fn=<AddBackward0>)\n",
      "epoch =  65 , loss =  tensor(346.7267, grad_fn=<AddBackward0>)\n",
      "epoch =  66 , loss =  tensor(346.0021, grad_fn=<AddBackward0>)\n",
      "epoch =  67 , loss =  tensor(345.2700, grad_fn=<AddBackward0>)\n",
      "epoch =  68 , loss =  tensor(344.5324, grad_fn=<AddBackward0>)\n",
      "epoch =  69 , loss =  tensor(343.7898, grad_fn=<AddBackward0>)\n",
      "epoch =  70 , loss =  tensor(343.0411, grad_fn=<AddBackward0>)\n",
      "epoch =  71 , loss =  tensor(342.2882, grad_fn=<AddBackward0>)\n",
      "epoch =  72 , loss =  tensor(341.5262, grad_fn=<AddBackward0>)\n",
      "epoch =  73 , loss =  tensor(340.7586, grad_fn=<AddBackward0>)\n",
      "epoch =  74 , loss =  tensor(339.9849, grad_fn=<AddBackward0>)\n",
      "epoch =  75 , loss =  tensor(339.2040, grad_fn=<AddBackward0>)\n",
      "epoch =  76 , loss =  tensor(338.4182, grad_fn=<AddBackward0>)\n",
      "epoch =  77 , loss =  tensor(337.6263, grad_fn=<AddBackward0>)\n",
      "epoch =  78 , loss =  tensor(336.8282, grad_fn=<AddBackward0>)\n",
      "epoch =  79 , loss =  tensor(336.0234, grad_fn=<AddBackward0>)\n",
      "epoch =  80 , loss =  tensor(335.2130, grad_fn=<AddBackward0>)\n",
      "epoch =  81 , loss =  tensor(334.3975, grad_fn=<AddBackward0>)\n",
      "epoch =  82 , loss =  tensor(333.5757, grad_fn=<AddBackward0>)\n",
      "epoch =  83 , loss =  tensor(332.7492, grad_fn=<AddBackward0>)\n",
      "epoch =  84 , loss =  tensor(331.9173, grad_fn=<AddBackward0>)\n",
      "epoch =  85 , loss =  tensor(331.0797, grad_fn=<AddBackward0>)\n",
      "epoch =  86 , loss =  tensor(330.2373, grad_fn=<AddBackward0>)\n",
      "epoch =  87 , loss =  tensor(329.3878, grad_fn=<AddBackward0>)\n",
      "epoch =  88 , loss =  tensor(328.5349, grad_fn=<AddBackward0>)\n",
      "epoch =  89 , loss =  tensor(327.6751, grad_fn=<AddBackward0>)\n",
      "epoch =  90 , loss =  tensor(326.8115, grad_fn=<AddBackward0>)\n",
      "epoch =  91 , loss =  tensor(325.9437, grad_fn=<AddBackward0>)\n",
      "epoch =  92 , loss =  tensor(325.0711, grad_fn=<AddBackward0>)\n",
      "epoch =  93 , loss =  tensor(324.1939, grad_fn=<AddBackward0>)\n",
      "epoch =  94 , loss =  tensor(323.3132, grad_fn=<AddBackward0>)\n",
      "epoch =  95 , loss =  tensor(322.4279, grad_fn=<AddBackward0>)\n",
      "epoch =  96 , loss =  tensor(321.5403, grad_fn=<AddBackward0>)\n",
      "epoch =  97 , loss =  tensor(320.6458, grad_fn=<AddBackward0>)\n",
      "epoch =  98 , loss =  tensor(319.7491, grad_fn=<AddBackward0>)\n",
      "epoch =  99 , loss =  tensor(318.8496, grad_fn=<AddBackward0>)\n",
      "epoch =  100 , loss =  tensor(317.9445, grad_fn=<AddBackward0>)\n",
      "epoch =  101 , loss =  tensor(317.0387, grad_fn=<AddBackward0>)\n",
      "epoch =  102 , loss =  tensor(316.1289, grad_fn=<AddBackward0>)\n",
      "epoch =  103 , loss =  tensor(315.2169, grad_fn=<AddBackward0>)\n",
      "epoch =  104 , loss =  tensor(314.3020, grad_fn=<AddBackward0>)\n",
      "epoch =  105 , loss =  tensor(313.3846, grad_fn=<AddBackward0>)\n",
      "epoch =  106 , loss =  tensor(312.4653, grad_fn=<AddBackward0>)\n",
      "epoch =  107 , loss =  tensor(311.5425, grad_fn=<AddBackward0>)\n",
      "epoch =  108 , loss =  tensor(310.6178, grad_fn=<AddBackward0>)\n",
      "epoch =  109 , loss =  tensor(309.6909, grad_fn=<AddBackward0>)\n",
      "epoch =  110 , loss =  tensor(308.7624, grad_fn=<AddBackward0>)\n",
      "epoch =  111 , loss =  tensor(307.8313, grad_fn=<AddBackward0>)\n",
      "epoch =  112 , loss =  tensor(306.8996, grad_fn=<AddBackward0>)\n",
      "epoch =  113 , loss =  tensor(305.9651, grad_fn=<AddBackward0>)\n",
      "epoch =  114 , loss =  tensor(305.0294, grad_fn=<AddBackward0>)\n",
      "epoch =  115 , loss =  tensor(304.0914, grad_fn=<AddBackward0>)\n",
      "epoch =  116 , loss =  tensor(303.1497, grad_fn=<AddBackward0>)\n",
      "epoch =  117 , loss =  tensor(302.2082, grad_fn=<AddBackward0>)\n",
      "epoch =  118 , loss =  tensor(301.2635, grad_fn=<AddBackward0>)\n",
      "epoch =  119 , loss =  tensor(300.3173, grad_fn=<AddBackward0>)\n",
      "epoch =  120 , loss =  tensor(299.3701, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  121 , loss =  tensor(298.4211, grad_fn=<AddBackward0>)\n",
      "epoch =  122 , loss =  tensor(297.4701, grad_fn=<AddBackward0>)\n",
      "epoch =  123 , loss =  tensor(296.5180, grad_fn=<AddBackward0>)\n",
      "epoch =  124 , loss =  tensor(295.5649, grad_fn=<AddBackward0>)\n",
      "epoch =  125 , loss =  tensor(294.6102, grad_fn=<AddBackward0>)\n",
      "epoch =  126 , loss =  tensor(293.6547, grad_fn=<AddBackward0>)\n",
      "epoch =  127 , loss =  tensor(292.6964, grad_fn=<AddBackward0>)\n",
      "epoch =  128 , loss =  tensor(291.7394, grad_fn=<AddBackward0>)\n",
      "epoch =  129 , loss =  tensor(290.7788, grad_fn=<AddBackward0>)\n",
      "epoch =  130 , loss =  tensor(289.8176, grad_fn=<AddBackward0>)\n",
      "epoch =  131 , loss =  tensor(288.8556, grad_fn=<AddBackward0>)\n",
      "epoch =  132 , loss =  tensor(287.8918, grad_fn=<AddBackward0>)\n",
      "epoch =  133 , loss =  tensor(286.9265, grad_fn=<AddBackward0>)\n",
      "epoch =  134 , loss =  tensor(285.9602, grad_fn=<AddBackward0>)\n",
      "epoch =  135 , loss =  tensor(284.9952, grad_fn=<AddBackward0>)\n",
      "epoch =  136 , loss =  tensor(284.0273, grad_fn=<AddBackward0>)\n",
      "epoch =  137 , loss =  tensor(283.0586, grad_fn=<AddBackward0>)\n",
      "epoch =  138 , loss =  tensor(282.0889, grad_fn=<AddBackward0>)\n",
      "epoch =  139 , loss =  tensor(281.1188, grad_fn=<AddBackward0>)\n",
      "epoch =  140 , loss =  tensor(280.1473, grad_fn=<AddBackward0>)\n",
      "epoch =  141 , loss =  tensor(279.1756, grad_fn=<AddBackward0>)\n",
      "epoch =  142 , loss =  tensor(278.2021, grad_fn=<AddBackward0>)\n",
      "epoch =  143 , loss =  tensor(277.2306, grad_fn=<AddBackward0>)\n",
      "epoch =  144 , loss =  tensor(276.2568, grad_fn=<AddBackward0>)\n",
      "epoch =  145 , loss =  tensor(275.2856, grad_fn=<AddBackward0>)\n",
      "epoch =  146 , loss =  tensor(274.3129, grad_fn=<AddBackward0>)\n",
      "epoch =  147 , loss =  tensor(273.3388, grad_fn=<AddBackward0>)\n",
      "epoch =  148 , loss =  tensor(272.3644, grad_fn=<AddBackward0>)\n",
      "epoch =  149 , loss =  tensor(271.3898, grad_fn=<AddBackward0>)\n",
      "epoch =  150 , loss =  tensor(270.4154, grad_fn=<AddBackward0>)\n",
      "epoch =  151 , loss =  tensor(269.4424, grad_fn=<AddBackward0>)\n",
      "epoch =  152 , loss =  tensor(268.4682, grad_fn=<AddBackward0>)\n",
      "epoch =  153 , loss =  tensor(267.4955, grad_fn=<AddBackward0>)\n",
      "epoch =  154 , loss =  tensor(266.5221, grad_fn=<AddBackward0>)\n",
      "epoch =  155 , loss =  tensor(265.5504, grad_fn=<AddBackward0>)\n",
      "epoch =  156 , loss =  tensor(264.5777, grad_fn=<AddBackward0>)\n",
      "epoch =  157 , loss =  tensor(263.6064, grad_fn=<AddBackward0>)\n",
      "epoch =  158 , loss =  tensor(262.6360, grad_fn=<AddBackward0>)\n",
      "epoch =  159 , loss =  tensor(261.6647, grad_fn=<AddBackward0>)\n",
      "epoch =  160 , loss =  tensor(260.6964, grad_fn=<AddBackward0>)\n",
      "epoch =  161 , loss =  tensor(259.7265, grad_fn=<AddBackward0>)\n",
      "epoch =  162 , loss =  tensor(258.7592, grad_fn=<AddBackward0>)\n",
      "epoch =  163 , loss =  tensor(257.7921, grad_fn=<AddBackward0>)\n",
      "epoch =  164 , loss =  tensor(256.8272, grad_fn=<AddBackward0>)\n",
      "epoch =  165 , loss =  tensor(255.8631, grad_fn=<AddBackward0>)\n",
      "epoch =  166 , loss =  tensor(254.8994, grad_fn=<AddBackward0>)\n",
      "epoch =  167 , loss =  tensor(253.9377, grad_fn=<AddBackward0>)\n",
      "epoch =  168 , loss =  tensor(252.9785, grad_fn=<AddBackward0>)\n",
      "epoch =  169 , loss =  tensor(252.0206, grad_fn=<AddBackward0>)\n",
      "epoch =  170 , loss =  tensor(251.0631, grad_fn=<AddBackward0>)\n",
      "epoch =  171 , loss =  tensor(250.1080, grad_fn=<AddBackward0>)\n",
      "epoch =  172 , loss =  tensor(249.1563, grad_fn=<AddBackward0>)\n",
      "epoch =  173 , loss =  tensor(248.2037, grad_fn=<AddBackward0>)\n",
      "epoch =  174 , loss =  tensor(247.2559, grad_fn=<AddBackward0>)\n",
      "epoch =  175 , loss =  tensor(246.3069, grad_fn=<AddBackward0>)\n",
      "epoch =  176 , loss =  tensor(245.3604, grad_fn=<AddBackward0>)\n",
      "epoch =  177 , loss =  tensor(244.4161, grad_fn=<AddBackward0>)\n",
      "epoch =  178 , loss =  tensor(243.4742, grad_fn=<AddBackward0>)\n",
      "epoch =  179 , loss =  tensor(242.5342, grad_fn=<AddBackward0>)\n",
      "epoch =  180 , loss =  tensor(241.5970, grad_fn=<AddBackward0>)\n",
      "epoch =  181 , loss =  tensor(240.6625, grad_fn=<AddBackward0>)\n",
      "epoch =  182 , loss =  tensor(239.7303, grad_fn=<AddBackward0>)\n",
      "epoch =  183 , loss =  tensor(238.8018, grad_fn=<AddBackward0>)\n",
      "epoch =  184 , loss =  tensor(237.8759, grad_fn=<AddBackward0>)\n",
      "epoch =  185 , loss =  tensor(236.9523, grad_fn=<AddBackward0>)\n",
      "epoch =  186 , loss =  tensor(236.0319, grad_fn=<AddBackward0>)\n",
      "epoch =  187 , loss =  tensor(235.1148, grad_fn=<AddBackward0>)\n",
      "epoch =  188 , loss =  tensor(234.1987, grad_fn=<AddBackward0>)\n",
      "epoch =  189 , loss =  tensor(233.2870, grad_fn=<AddBackward0>)\n",
      "epoch =  190 , loss =  tensor(232.3775, grad_fn=<AddBackward0>)\n",
      "epoch =  191 , loss =  tensor(231.4710, grad_fn=<AddBackward0>)\n",
      "epoch =  192 , loss =  tensor(230.5684, grad_fn=<AddBackward0>)\n",
      "epoch =  193 , loss =  tensor(229.6682, grad_fn=<AddBackward0>)\n",
      "epoch =  194 , loss =  tensor(228.7716, grad_fn=<AddBackward0>)\n",
      "epoch =  195 , loss =  tensor(227.8803, grad_fn=<AddBackward0>)\n",
      "epoch =  196 , loss =  tensor(226.9913, grad_fn=<AddBackward0>)\n",
      "epoch =  197 , loss =  tensor(226.1052, grad_fn=<AddBackward0>)\n",
      "epoch =  198 , loss =  tensor(225.2231, grad_fn=<AddBackward0>)\n",
      "epoch =  199 , loss =  tensor(224.3445, grad_fn=<AddBackward0>)\n",
      "Prediction :  ['In', 'and', 'are', 'by']\n"
     ]
    }
   ],
   "source": [
    "## using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "EPOCHS = 200\n",
    "CONTEXT_SIZE = 4\n",
    "\n",
    "with open('../[01]data_set/data_set.txt', 'r') as file: \n",
    "    example_sentence = file.readline()\n",
    "\n",
    "example_sentence = example_sentence.split()\n",
    "print(example_sentence)\n",
    "\n",
    "\n",
    "\n",
    "#(1) 입력받은 문장을 단어로 쪼개고, 중복을 제거해줍니다.\n",
    "vocab = set(example_sentence)\n",
    "vocab_size = len(example_sentence)\n",
    "\n",
    "#(2) 단어 : 인덱스, 인덱스 : 단어를 가지는 딕셔너리를 선언해 줍니다.\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for index, word in enumerate(vocab)}\n",
    "\n",
    "#(3) 학습을 위한 데이터를 생성해 줍니다.\n",
    "data = make_data(example_sentence)\n",
    "\n",
    "\n",
    "\n",
    "#(5) 모델을 선언해주고, loss function, optimizer등을 선언해줍니다.\n",
    "model = SKIP_GRAM(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "#(6) 학습을 진행합니다.\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_index)  \n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_index[t] for t in target]))\n",
    "    print('epoch = ',epoch, ', loss = ',total_loss)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#﻿(7) test하고 싶은 문장을 뽑고, test를 진행합니다.\n",
    "test_data = 'Skip-Gram'\n",
    "test_vector = make_context_vector(test_data, word_to_index)\n",
    "result = model(test_vector)\n",
    "print('Prediction : ', [index_to_word[torch.argmax(r).item()] for r in result])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
